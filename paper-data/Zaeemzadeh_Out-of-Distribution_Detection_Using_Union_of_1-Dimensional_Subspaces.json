{
  "arXiv": null,
  "title": "Out-of-Distribution Detection Using Union of 1-Dimensional Subspaces",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Zaeemzadeh_Out-of-Distribution_Detection_Using_Union_of_1-Dimensional_Subspaces_CVPR_2021_paper.pdf",
  "authors": [
    "Alireza Zaeemzadeh",
    "Niccolo Bisagno",
    "Zeno Sambugaro",
    "Nicola Conci",
    "Nazanin Rahnavard",
    "Mubarak Shah"
  ],
  "abstract": "The goal of out-of-distribution (OOD) detection is to handle the situations where the test samples are drawn from a different distribution than the training data. In this paper, we argue that OOD samples can be detected more easily if the training data is embedded into a low-dimensional space, such that the embedded training samples lie on a union of 1-dimensional subspaces. We show that such embedding of the in-distribution (ID) samples provides us with two main advantages. First, due to compact representation in the feature space, OOD samples are less likely to occupy the same region as the known classes. Second, the first singular vector of ID samples belonging to a 1-dimensional subspace can be used as their robust representative. Motivated by these observations, we train a deep neural network such that the ID samples are embedded onto a union of 1-dimensional subspaces. At the test time, employing sampling techniques used for approximate Bayesian inference in deep learning, input samples are detected as OOD if they occupy the region corresponding to the ID samples with probability 0. Spectral components of the ID samples are used as robust representative of this region. Our method does not have any hyperparameter to be tuned using extra information and it can be applied on different modalities with minimal change. The effectiveness of the proposed method is demonstrated on different benchmark datasets, both in the image and video classification domains.",
  "s2id": "a43f7d6a751a6ad8667272f1176d2f15dbd8feb6",
  "twitter": {
    "retweets": 0,
    "likes": 0,
    "replies": 0
  },
  "citations": 0
}