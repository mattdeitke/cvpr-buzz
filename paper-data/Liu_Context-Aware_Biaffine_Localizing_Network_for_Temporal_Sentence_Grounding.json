{
  "arXiv": "http://arxiv.org/abs/2103.11555",
  "title": "Context-Aware Biaffine Localizing Network for Temporal Sentence Grounding",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Context-Aware_Biaffine_Localizing_Network_for_Temporal_Sentence_Grounding_CVPR_2021_paper.pdf",
  "authors": [
    "Daizong Liu",
    "Xiaoye Qu",
    "Jianfeng Dong",
    "Pan Zhou",
    "Yu Cheng",
    "Wei Wei",
    "Zichuan Xu",
    "Yulai Xie"
  ],
  "abstract": "This paper addresses the problem of temporal sentence grounding (TSG), which aims to identify the temporal boundary of a specific segment from an untrimmed video by a sentence query. Previous works either compare pre-defined candidate segments with the query and select the best one by ranking, or directly regress the boundary timestamps of the target segment. In this paper, we propose a novel localization framework that scores all pairs of start and end indices within the video simultaneously with a biaffine mechanism. In particular, we present a Context-aware Biaffine Localizing Network (CBLN) which incorporates both local and global contexts into features of each start/end position for biaffine-based localization. The local contexts from the adjacent frames help distinguish the visually similar appearance, and the global contexts from the entire video contribute to reasoning the temporal relation. Besides, we also develop a multi-modal self-attention module to provide fine-grained query-guided video representation for this biaffine strategy. Extensive experiments show that our CBLN significantly outperforms state-of-the-arts on three public datasets (ActivityNet Captions, TACoS, and Charades-STA), demonstrating the effectiveness of the proposed localization framework.",
  "s2id": "906af0b014dc86e9fb232a983c5a2f8fc712743b",
  "twitter": {
    "retweets": 0,
    "likes": 0,
    "replies": 0,
    "ids": [
      "1374385714607976462",
      "1374219483926175744",
      "1374174347003318279"
    ]
  },
  "citations": 0,
  "posterSession": "Wednesday"
}