{
  "arXiv": "http://arxiv.org/abs/2103.16129",
  "title": "Self-Guided and Cross-Guided Learning for Few-Shot Segmentation",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Self-Guided_and_Cross-Guided_Learning_for_Few-Shot_Segmentation_CVPR_2021_paper.pdf",
  "authors": [
    "Bingfeng Zhang",
    "Jimin Xiao",
    "Terry Qin"
  ],
  "abstract": "Few-shot segmentation has been attracting a lot of attention due to its effectiveness to segment unseen object classes with a few annotated samples. Most existing approaches use masked Global Average Pooling (GAP) to encode an annotated support image to a feature vector to facilitate query image segmentation. However, this pipeline unavoidably loses some discriminative information due to the average operation. In this paper, we propose a simple but effective self-guided learning approach, where the lost critical information is mined. Specifically, through making an initial prediction for the annotated support image, the covered and uncovered foreground regions are encoded to the primary and auxiliary support vectors using masked GAP, respectively. By aggregating both the primary and auxiliary support vectors, better segmentation performance is obtained on query images. Enlightened by our self-guided module for 1-shot segmentation, we propose a cross-guided module for multiple shot segmentation, where the final mask is fused using predictions from multiple annotated samples with high-quality support vectors contributing more and vice versa. This module improves the final prediction in the inference stage without re-training. Extensive experiments show that our approach achieves new state-of-the-art performances on both PASCAL-5i and COCO-20i datasets. Source code will be released once the paper is accepted.",
  "s2id": "50eb5344f5a7cbc9b40b8a515cc941a29e8b4f41",
  "citations": 0,
  "posterSession": "Wednesday"
}