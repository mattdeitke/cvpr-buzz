{
  "arXiv": "http://arxiv.org/abs/2105.05226",
  "title": "Home Action Genome: Cooperative Compositional Action Understanding",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Rai_Home_Action_Genome_Cooperative_Compositional_Action_Understanding_CVPR_2021_paper.pdf",
  "authors": [
    "Nishant Rai",
    "Haofeng Chen",
    "Jingwei Ji",
    "Rishi Desai",
    "Kazuki Kozuka",
    "Shun Ishizaka",
    "Ehsan Adeli",
    "Juan Carlos Niebles"
  ],
  "abstract": "Existing research on action recognition treats activities as monolithic events occurring in videos. Recently, the benefits of formulating actions as a combination of atomic-actions have shown promise in improving action understanding with the emergence of datasets containing such annotations, allowing us to learn representations capturing this information. However, there remains a lack of studies that extend action composition and leverage multiple viewpoints and multiple modalities of data for representation learning. To promote research in this direction, we introduce Home Action Genome (HOMAGE): a multi-view action dataset with multiple modalities and view-points supplemented with hierarchical activity and atomic action labels together with dense scene composition labels. Leveraging rich multi-modal and multi-view settings, we propose Cooperative Compositional Action Understanding (CCAU), a cooperative learning framework for hierarchical action recognition that is aware of compositional action elements. CCAU shows consistent performance improvements across all modalities. Furthermore, we demonstrate the utility of co-learning compositions in few-shot action recognition by achieving 28.6% mAP with just a single sample.",
  "s2id": "453170a3fada768913a84b9ed52951e3299c9d03",
  "twitter": {
    "retweets": 21,
    "likes": 73,
    "replies": 0
  },
  "citations": 0
}