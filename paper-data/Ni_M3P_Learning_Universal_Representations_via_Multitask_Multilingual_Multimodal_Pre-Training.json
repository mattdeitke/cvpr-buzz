{
  "arXiv": "http://arxiv.org/abs/2006.02635",
  "title": "M3P: Learning Universal Representations via Multitask Multilingual Multimodal Pre-Training",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Ni_M3P_Learning_Universal_Representations_via_Multitask_Multilingual_Multimodal_Pre-Training_CVPR_2021_paper.pdf",
  "authors": [
    "Minheng Ni",
    "Haoyang Huang",
    "Lin Su",
    "Edward Cui",
    "Taroon Bharti",
    "Lijuan Wang",
    "Dongdong Zhang",
    "Nan Duan"
  ],
  "abstract": "We present M3P, a Multitask Multilingual Multimodal Pre-trained model that combines multilingual pre-training and multimodal pre-training into a unified framework via multitask pre-training. Our goal is to learn universal representations that can map objects occurred in different modalities or texts expressed in different languages into a common semantic space. In addition, to explicitly encourage fine-grained alignment between images and non-English languages, we also propose Multimodal Code-switched Training (MCT) to combine monolingual pre-training and multimodal pre-training via a code-switch strategy. Experiments are performed on the multilingual image retrieval task across two benchmark datasets, including MSCOCO and Multi30K. M3P can achieve comparable results for English and new state-of-the-art results for non-English languages.",
  "s2id": "9960d13255259c66812dd130e08c17720ff60c18",
  "twitter": {
    "retweets": 2,
    "likes": 20,
    "replies": 2
  },
  "citations": 5,
  "posterSession": "Tuesday"
}