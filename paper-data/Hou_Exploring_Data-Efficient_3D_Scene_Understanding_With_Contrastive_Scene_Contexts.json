{
  "arXiv": "http://arxiv.org/abs/2012.09165",
  "title": "Exploring Data-Efficient 3D Scene Understanding With Contrastive Scene Contexts",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Hou_Exploring_Data-Efficient_3D_Scene_Understanding_With_Contrastive_Scene_Contexts_CVPR_2021_paper.pdf",
  "authors": [
    "Ji Hou",
    "Benjamin Graham",
    "Matthias Niessner",
    "Saining Xie"
  ],
  "abstract": "The rapid progress in 3D scene understanding has come with growing demand for data; however, collecting and annotating 3D scenes (e.g. point clouds) are notoriously hard. For example, the number of scenes (e.g. indoor rooms) that can be accessed and scanned might be limited; even given sufficient data, acquiring 3D labels (e.g. instance masks) requires intensive human labor. In this paper, we explore data-efficient learning for 3D point cloud. As a first step towards this direction, we propose Contrastive Scene Contexts, a 3D pre-training method that makes use of both point-level correspondences and spatial contexts in a scene. Our method achieves state-of-the-art results on a suite of benchmarks where training data or labels are scarce. Our study reveals that exhaustive labelling of 3D point clouds might be unnecessary; and remarkably, on ScanNet, even using 0.1% of point labels, we still achieve 89% (instance segmentation) and 96% (semantic segmentation) of the baseline performance that uses full annotations.",
  "s2id": "053b96e162724fa10d8639196a99856d60de5214",
  "twitter": {
    "retweets": 26,
    "likes": 82,
    "replies": 0
  },
  "citations": 4,
  "posterSession": "Friday"
}