{
  "arXiv": "http://arxiv.org/abs/2105.06818",
  "title": "Collaborative Spatial-Temporal Modeling for Language-Queried Video Actor Segmentation",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Hui_Collaborative_Spatial-Temporal_Modeling_for_Language-Queried_Video_Actor_Segmentation_CVPR_2021_paper.pdf",
  "authors": [
    "Tianrui Hui",
    "Shaofei Huang",
    "Si Liu",
    "Zihan Ding",
    "Guanbin Li",
    "Wenguan Wang",
    "Jizhong Han",
    "Fei Wang"
  ],
  "abstract": "Language-queried video actor segmentation aims to predict the pixel-level mask of the actor which performs the actions described by a natural language query in the target frames. Existing methods adopt 3D CNNs over the video clip as a general encoder to extract a mixed spatio-temporal feature for the target frame. Though 3D convolutions are amenable to recognizing which actor is performing the queried actions, it also inevitably introduces misaligned spatial information from adjacent frames, which confuses features of the target frame and yields inaccurate segmentation. Therefore, we propose a collaborative spatial-temporal encoder-decoder framework which contains a 3D temporal encoder over the video clip to recognize the queried actions, and a 2D spatial encoder over the target frame to accurately segment the queried actors. In the decoder, a Language-Guided Feature Selection (LGFS) module is proposed to flexibly integrate spatial and temporal features from the two encoders. We also propose a Cross-Modal Adaptive Modulation (CMAM) module to dynamically recombine spatial- and temporal-relevant linguistic features for multimodal feature interaction in each stage of the two encoders. Our method achieves new state-of-the-art performance on two popular benchmarks with less computational overhead than previous approaches.",
  "s2id": "0c078c661331b41a539a5dbdec653f6686963d74",
  "citations": 0,
  "posterSession": "Tuesday"
}