{
  "arXiv": "http://arxiv.org/abs/2103.07289",
  "title": "Searching by Generating: Flexible and Efficient One-Shot NAS With Architecture Generator",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Huang_Searching_by_Generating_Flexible_and_Efficient_One-Shot_NAS_With_Architecture_CVPR_2021_paper.pdf",
  "authors": [
    "Sian-Yao Huang",
    "Wei-Ta Chu"
  ],
  "abstract": "In one-shot NAS, sub-networks need to be searched from the supernet to meet different hardware constraints. However, the search cost is high and N times of searches are needed for N different constraints. In this work, we propose a novel search strategy called architecture generator to search sub-networks by generating them, so that the search process can be much more efficient and flexible. With the trained architecture generator, given target hardware constraints as the input, N good architectures can be generated for N constraints by just one forward pass without re-searching and supernet retraining. Moreover, we propose a novel single-path supernet, called unified supernet, to further improve search efficiency and reduce GPU memory consumption of the architecture generator. With the architecture generator and the unified supernet, we propose a flexible and efficient one-shot NAS framework, called Searching by Generating NAS (SGNAS). With the pre-trained supernt, the search time of SGNAS for N different hardware constraints is only 5 GPU hours, which is 4N times faster than previous SOTA single-path methods. After training from scratch, the top1-accuracy of SGNAS on ImageNet is 77.1%, which is comparable with the SOTAs. The code is available at: https://github.com/eric8607242/SGNAS.",
  "s2id": "b978aac95a122f6cb120b1949224f4bbdb62b483",
  "twitter": {
    "retweets": 9,
    "likes": 16,
    "replies": 2,
    "ids": [
      "1371268982972514314",
      "1373129155894730752",
      "1371355850158206978"
    ]
  },
  "citations": 0,
  "posterSession": "Monday"
}