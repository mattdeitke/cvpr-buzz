{
  "arXiv": null,
  "title": "De-Rendering the World's Revolutionary Artefacts",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Wu_De-Rendering_the_Worlds_Revolutionary_Artefacts_CVPR_2021_paper.pdf",
  "authors": [
    "Shangzhe Wu",
    "Ameesh Makadia",
    "Jiajun Wu",
    "Noah Snavely",
    "Richard Tucker",
    "Angjoo Kanazawa"
  ],
  "abstract": "Recent works have shown exciting results in unsupervised image de-rendering--learning to decompose 3D shape, appearance, and lighting from single-image collections without explicit supervision. However, many of these assume simplistic material and lighting models. We propose a method, termed RADAR, that can recover environment illumination and surface materials from real single-image collections, relying neither on explicit 3D supervision, nor on multi-view or multi-light images. Specifically, we focus on rotationally symmetric artefacts that exhibit challenging surface properties including specular reflections, such as vases. We introduce a novel self-supervised albedo discriminator, which allows the model to recover plausible albedo without requiring any ground-truth during training. In conjunction with a shape reconstruction module exploiting rotational symmetry, we present an end-to-end learning framework that is able to de-render the world's revolutionary artefacts. We conduct experiments on a real vase dataset and demonstrate compelling decomposition results, allowing for applications including free-viewpoint rendering and relighting. More results and code at: https://sorderender.github.io/.",
  "s2id": "053e3362b4d74c09ad72cc8731832211df420e19",
  "twitter": {
    "retweets": 19,
    "likes": 65,
    "replies": 0
  },
  "citations": 0
}