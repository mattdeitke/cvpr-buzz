{
  "arXiv": "http://arxiv.org/abs/2103.13137",
  "title": "Learning Salient Boundary Feature for Anchor-free Temporal Action Localization",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Lin_Learning_Salient_Boundary_Feature_for_Anchor-free_Temporal_Action_Localization_CVPR_2021_paper.pdf",
  "authors": [
    "Chuming Lin",
    "Chengming Xu",
    "Donghao Luo",
    "Yabiao Wang",
    "Ying Tai",
    "Chengjie Wang",
    "Jilin Li",
    "Feiyue Huang",
    "Yanwei Fu"
  ],
  "abstract": "Temporal action localization is an important yet challenging task in video understanding. Typically, such a task aims at inferring both the action category and localization of the start and end frame for each action instance in a long, untrimmed video. While most current models achieve good results by using pre-defined anchors and numerous actionness, such methods could be bothered with both large number of outputs and heavy tuning of locations and sizes corresponding to different anchors. Instead, anchor-free methods is lighter, getting rid of redundant hyper-parameters, but gains few attention. In this paper, we propose the first purely anchor-free temporal localization method, which is both efficient and effective. Our model includes (i) an end-to-end trainable basic predictor, (ii) a saliency-based refinement module to gather more valuable boundary features for each proposal with a novel boundary pooling, and (iii) several consistency constraints to make sure our model can find the accurate boundary given arbitrary proposals. Extensive experiments show that our method beats all anchor-based and actionness-guided methods with a remarkable margin on THUMOS14, achieving state-of-the-art results, and comparable ones on ActivityNet v1.3. Our code will be made available upon publication.",
  "s2id": "d9a447690ba5845cc466f42e96962ee3aee732eb",
  "citations": 0
}