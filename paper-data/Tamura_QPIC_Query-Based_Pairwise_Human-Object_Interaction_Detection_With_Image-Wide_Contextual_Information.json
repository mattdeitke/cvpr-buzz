{
  "arXiv": "http://arxiv.org/abs/2103.05399",
  "title": "QPIC: Query-Based Pairwise Human-Object Interaction Detection With Image-Wide Contextual Information",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Tamura_QPIC_Query-Based_Pairwise_Human-Object_Interaction_Detection_With_Image-Wide_Contextual_Information_CVPR_2021_paper.pdf",
  "authors": [
    "Masato Tamura",
    "Hiroki Ohashi",
    "Tomoaki Yoshinaga"
  ],
  "abstract": "We propose a simple, intuitive yet powerful method for human-object interaction (HOI) detection. HOIs are so diverse in spatial distribution in an image that existing CNN-based methods face the following three major drawbacks; they cannot leverage image-wide features due to CNN's locality, they rely on a manually defined location-of-interest for the feature aggregation, which sometimes does not cover contextually important regions, and they cannot help but mix up the features for multiple HOI instances if they are located closely. To overcome these drawbacks, we propose a transformer-based feature extractor, in which an attention mechanism and query-based detection play key roles. The attention mechanism is effective in aggregating contextually important information image-wide, while the queries, which we design in such a way that each query captures at most one human-object pair, can avoid mixing up the features from multiple instances. This transformer-based feature extractor produces so effective embeddings that the subsequent detection heads may be fairly simple and intuitive. The extensive analysis reveals that the proposed method successfully extracts contextually important features, and thus outperforms existing methods by large margins (5.37 mAP on HICO-DET, and 5.6 mAP on V-COCO). The source codes are available at https://github.com/hitachi-rd-cv/qpic.",
  "s2id": "b6d6223bfa9de61d675d2fd4dd43c5924f393a55",
  "twitter": {
    "retweets": 6,
    "likes": 27,
    "replies": 0
  },
  "citations": 1,
  "posterSession": "Wednesday"
}