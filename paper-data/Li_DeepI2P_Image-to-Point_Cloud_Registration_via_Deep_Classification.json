{
  "arXiv": "http://arxiv.org/abs/2104.03501",
  "title": "DeepI2P: Image-to-Point Cloud Registration via Deep Classification",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Li_DeepI2P_Image-to-Point_Cloud_Registration_via_Deep_Classification_CVPR_2021_paper.pdf",
  "authors": [
    "Jiaxin Li",
    "Gim Hee Lee"
  ],
  "abstract": "This paper presents DeepI2P: a novel approach for cross-modality registration between an image and a point cloud. Given an image (e.g. from a rgb-camera) and a general point cloud (e.g. from a 3D Lidar scanner) captured at different locations in the same scene, our method estimates the relative rigid transformation between the coordinate frames of the camera and Lidar. Learning common feature descriptors to establish correspondences for the registration is inherently challenging due to the lack of appearance and geometric correlations across the two modalities. We circumvent the difficulty by converting the registration problem into a classification and inverse camera projection optimization problem. A classification neural network is designed to label whether the projection of each point in the point cloud is within or beyond the camera frustum. These labeled points are subsequently passed into a novel inverse camera projection solver to estimate the relative pose. Extensive experimental results on Oxford Robotcar and KITTI datasets demonstrate the feasibility of our approach. Our source code is available at https://github.com/lijx10/DeepI2P",
  "s2id": "b8277ce2b880b97fa251057c14003aacec2d7a9b",
  "twitter": {
    "retweets": 0,
    "likes": 0,
    "replies": 1,
    "ids": [
      "1381154764973608961",
      "1380693106148925442",
      "1380375678592245762"
    ]
  },
  "citations": 0,
  "posterSession": "Friday"
}