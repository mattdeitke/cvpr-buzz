{
  "arXiv": "http://arxiv.org/abs/2103.03454",
  "title": "Structured Scene Memory for Vision-Language Navigation",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Structured_Scene_Memory_for_Vision-Language_Navigation_CVPR_2021_paper.pdf",
  "authors": [
    "Hanqing Wang",
    "Wenguan Wang",
    "Wei Liang",
    "Caiming Xiong",
    "Jianbing Shen"
  ],
  "abstract": "Recently, numerous algorithms have been developed to tackle the problem of vision-language navigation (VLN), i.e., entailing an agent to navigate 3D environments through following linguistic instructions. However, current VLN agents simply store their past experiences/observations as latent states in recurrent networks, failing to capture environment layouts and make long-term planning. To address these limitations, we propose a crucial architecture, called Structured Scene Memory (SSM). It is compartmentalized enough to accurately memorize the percepts during navigation. It also serves as a structured scene representation, which captures and disentangles visual and geometric cues in the environment. SSM has a collect-read controller that adaptively collects information for supporting current decision making and mimics iterative algorithms for long-range reasoning. As SSM provides a complete action space, i.e., all the navigable places on the map, a frontier-exploration based navigation decision making strategy is introduced to enable efficient and global planning. Experiment results on two VLN datasets (i.e., R2R and R4R) show that our method achieves state-of-the-art performance on several metrics.",
  "s2id": "ee0de77c9bbe4a60fdbba09e2d37b3db842312df",
  "twitter": {
    "retweets": 3,
    "likes": 21,
    "replies": 1,
    "ids": [
      "1368761252298846208",
      "1370558515731906563"
    ]
  },
  "citations": 1,
  "posterSession": "Wednesday"
}