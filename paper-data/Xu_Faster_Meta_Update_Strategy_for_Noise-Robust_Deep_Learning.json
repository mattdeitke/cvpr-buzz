{
  "arXiv": "http://arxiv.org/abs/2104.15092",
  "title": "Faster Meta Update Strategy for Noise-Robust Deep Learning",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Xu_Faster_Meta_Update_Strategy_for_Noise-Robust_Deep_Learning_CVPR_2021_paper.pdf",
  "authors": [
    "Youjiang Xu",
    "Linchao Zhu",
    "Lu Jiang",
    "Yi Yang"
  ],
  "abstract": "It has been shown that deep neural networks are prone to overfitting on biased training data. Towards addressing this issue, meta-learning employs a meta model for correcting the training bias. Despite the promising performances, super slow training is currently the bottleneck in the meta learning approaches. In this paper, we introduce a novel Faster Meta Update Strategy (FaMUS) to replace the most expensive step in the meta gradient computation with a faster layer-wise approximation. We empirically find that FaMUS yields not only a reasonably accurate but also a low-variance approximation of the meta gradient. We conduct extensive experiments to verify the proposed method on two tasks. We show our method is able to save two-thirds of the training time while still maintaining the comparable or achieving even better generalization performance. In particular, our method achieves the state-of-the-art performance on both synthetic and realistic noisy labels, and obtains promising performance on long-tailed recognition on standard benchmarks. Code are released at https://github.com/youjiangxu/FaMUS.",
  "s2id": "598b2441396f5287b462bf1684ffc4be5ec8d800",
  "twitter": {
    "retweets": 10,
    "likes": 20,
    "replies": 2
  },
  "citations": 2,
  "posterSession": "Monday"
}