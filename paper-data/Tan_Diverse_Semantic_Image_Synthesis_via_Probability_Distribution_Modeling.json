{
  "arXiv": "http://arxiv.org/abs/2103.06878",
  "title": "Diverse Semantic Image Synthesis via Probability Distribution Modeling",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Tan_Diverse_Semantic_Image_Synthesis_via_Probability_Distribution_Modeling_CVPR_2021_paper.pdf",
  "authors": [
    "Zhentao Tan",
    "Menglei Chai",
    "Dongdong Chen",
    "Jing Liao",
    "Qi Chu",
    "Bin Liu",
    "Gang Hua",
    "Nenghai Yu"
  ],
  "abstract": "Semantic image synthesis, translating semantic layouts to photo-realistic images, is a one-to-many mapping problem. Though impressive progress has been recently made, diverse semantic synthesis that can efficiently produce semantic-level multimodal results, still remains a challenge. In this paper, we propose a novel diverse semantic image synthesis framework from the perspective of semantic class distributions, which naturally supports diverse generation at semantic or even instance level. We achieve this by modeling class-level conditional modulation parameters as continuous probability distributions instead of discrete values, and sampling per-instance modulation parameters through instance-adaptive stochastic sampling that is consistent across the network. Moreover, we propose prior noise remapping, through linear perturbation parameters encoded from paired references, to facilitate supervised training and exemplar-based instance style control at test time. Extensive experiments on multiple datasets show that our method can achieve superior diversity and comparable quality compared to state-of-the-art methods. Code will be available at https://github.com/tzt101/INADE.git",
  "s2id": "b115783455ef4d9f01e32e207ef15ca4623b4a5f",
  "twitter": {
    "retweets": 2,
    "likes": 17,
    "replies": 0
  },
  "citations": 0
}