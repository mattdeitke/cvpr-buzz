{
  "arXiv": "http://arxiv.org/abs/2011.11108",
  "title": "Multiresolution Knowledge Distillation for Anomaly Detection",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Salehi_Multiresolution_Knowledge_Distillation_for_Anomaly_Detection_CVPR_2021_paper.pdf",
  "authors": [
    "Mohammadreza Salehi",
    "Niousha Sadjadi",
    "Soroosh Baselizadeh",
    "Mohammad H. Rohban",
    "Hamid R. Rabiee"
  ],
  "abstract": "Unsupervised representation learning has proved to be a critical component of anomaly detection/localization in images. The challenges to learn such a representation are two-fold. Firstly, the sample size is not often large enough to learn a rich generalizable representation through conventional techniques. Secondly, while only normal samples are available at training, the learned features should be discriminative of normal and anomalous samples. Here, we propose to use the \"distillation\" of features at various layers of an expert network, which is pre-trained on ImageNet, into a simpler cloner network to tackle both issues. We detect and localize anomalies using the discrepancy between the expert and cloner networks' intermediate activation values given an input sample. We show that considering multiple intermediate hints in distillation leads to better exploitation of the expert's knowledge and a more distinctive discrepancy between the two networks, compared to utilizing only the last layer activation values. Notably, previous methods either fail in precise anomaly localization or need expensive region-based training. In contrast, with no need for any special or intensive training procedure, we incorporate interpretability algorithms in our novel framework to localize anomalous regions. Despite the striking difference between some test datasets and ImageNet, we achieve competitive or significantly superior results compared to SOTA on MNIST, F-MNIST, CIFAR-10, MVTecAD, Retinal-OCT, and two other medical datasets on both anomaly detection and localization.",
  "s2id": "6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d",
  "twitter": {
    "retweets": 2,
    "likes": 41,
    "replies": 1
  },
  "citations": 2
}