{
  "arXiv": "http://arxiv.org/abs/2104.12378",
  "title": "Delving into Data: Effectively Substitute Training for Black-box Attack",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Delving_into_Data_Effectively_Substitute_Training_for_Black-box_Attack_CVPR_2021_paper.pdf",
  "authors": [
    "Wenxuan Wang",
    "Bangjie Yin",
    "Taiping Yao",
    "Li Zhang",
    "Yanwei Fu",
    "Shouhong Ding",
    "Jilin Li",
    "Feiyue Huang",
    "Xiangyang Xue"
  ],
  "abstract": "Deep models have shown their vulnerability when processing adversarial samples. As for the black-box attack, without access to the architecture and weights of the attacked model, training a substitute model for adversarial attacks has attracted wide attention. Previous substitute training approaches focus on stealing the knowledge of the target model based on real training data or synthetic data, without exploring what kind of data can further improve the transferability between the substitute and target models. In this paper, we propose a novel perspective substitute training that focuses on designing the distribution of data used in the knowledge stealing process. More specifically, a diverse data generation module is proposed to synthesize large-scale data with wide distribution. And adversarial substitute training strategy is introduced to focus on the data distributed near the decision boundary. The combination of these two modules can further boost the consistency of the substitute model and target model, which greatly improves the effectiveness of adversarial attack. Extensive experiments demonstrate the efficacy of our method against state-of-the-art competitors under non-target and target attack settings. Detailed visualization and analysis are also provided to help understand the advantage of our method.",
  "s2id": "62c27f0432bb76362c8adac7ee0da06cbfa83409",
  "twitter": {
    "retweets": 0,
    "likes": 3,
    "replies": 0
  },
  "citations": 0,
  "posterSession": "Tuesday"
}