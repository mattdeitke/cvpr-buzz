{
  "arXiv": "http://arxiv.org/abs/2011.14565",
  "title": "Deep Implicit Templates for 3D Shape Representation",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Zheng_Deep_Implicit_Templates_for_3D_Shape_Representation_CVPR_2021_paper.pdf",
  "authors": [
    "Zerong Zheng",
    "Tao Yu",
    "Qionghai Dai",
    "Yebin Liu"
  ],
  "abstract": "Deep implicit functions (DIFs), as a kind of 3D shape representation, are becoming more and more popular in the 3D vision community due to their compactness and strong representation power. However, unlike polygon mesh-based templates, it remains a challenge to reason dense correspondences or other semantic relationships across shapes represented by DIFs, which limits its applications in texture transfer, shape analysis and so on. To overcome this limitation and also make DIFs more interpretable, we propose Deep Implicit Templates, a new 3D shape representation that supports explicit correspondence reasoning in deep implicit representations. Our key idea is to formulate DIFs as conditional deformations of a template implicit function. To this end, we propose Spatial Warping LSTM, which decomposes the conditional spatial transformation into multiple point-wise transformations and guarantees generalization capability. Moreover, the training loss is carefully designed in order to achieve high reconstruction accuracy while learning a plausible template with accurate correspondences in an unsupervised manner. Experiments show that our method can not only learn a common implicit template for a collection of shapes, but also establish dense correspondences across all the shapes simultaneously without any supervision.",
  "s2id": "d40382399fdf88488b0ffe2b6930791c1d72260c",
  "twitter": {
    "retweets": 2,
    "likes": 10,
    "replies": 0,
    "ids": [
      "1334604180636495872",
      "1359140347821977604"
    ]
  },
  "citations": 2,
  "posterSession": "Monday"
}