{
  "arXiv": "http://arxiv.org/abs/2104.00681",
  "title": "NeuralRecon: Real-Time Coherent 3D Reconstruction From Monocular Video",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Sun_NeuralRecon_Real-Time_Coherent_3D_Reconstruction_From_Monocular_Video_CVPR_2021_paper.pdf",
  "authors": [
    "Jiaming Sun",
    "Yiming Xie",
    "Linghao Chen",
    "Xiaowei Zhou",
    "Hujun Bao"
  ],
  "abstract": "We present a novel framework named NeuralRecon for real-time 3D scene reconstruction from a monocular video. Unlike previous methods that estimate single-view depth maps separately on each key-frame and fuse them later, we propose to directly reconstruct local surfaces represented as sparse TSDF volumes for each video fragment sequentially by a neural network. A learning-based TSDF fusion module based on gated recurrent units is used to guide the network to fuse features from previous fragments. This design allows the network to capture local smoothness prior and global shape prior of 3D surfaces when sequentially reconstructing the surfaces, resulting in accurate, coherent, and real-time surface reconstruction. The experiments on ScanNet and 7-Scenes datasets show that our system outperforms state-of-the-art methods in terms of both accuracy and speed. To the best of our knowledge, this is the first learning-based system that is able to reconstruct dense coherent 3D geometry in real-time. Code is available at the project page: https://zju3dv.github.io/neuralrecon/.",
  "s2id": "8db0bf0fa406254d4cd57eb413443a0b96bd12b8",
  "twitter": {
    "retweets": 65,
    "likes": 305,
    "replies": 8
  },
  "citations": 0,
  "posterSession": "Friday"
}