{
  "arXiv": null,
  "title": "Ego-Exo: Transferring Visual Representations From Third-Person to First-Person Videos",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Ego-Exo_Transferring_Visual_Representations_From_Third-Person_to_First-Person_Videos_CVPR_2021_paper.pdf",
  "authors": [
    "Yanghao Li",
    "Tushar Nagarajan",
    "Bo Xiong",
    "Kristen Grauman"
  ],
  "abstract": "We introduce an approach for pre-training egocentric video models using large-scale third-person video datasets. Learning from purely egocentric data is limited by low dataset scale and diversity, while using purely exocentric (third-person) data introduces a large domain mismatch. Our idea is to discover latent signals in third-person video that are predictive of key egocentric-specific properties. Incorporating these signals as knowledge distillation losses during pre-training results in models that benefit from both the scale and diversity of third-person video data, as well as representations that capture salient egocentric properties. Our experiments show that our Ego-Exo framework can be seamlessly integrated into standard video models; it outperforms all baselines when fine-tuned for egocentric activity recognition, achieving state-of-the-art results on Charades-Ego and EPIC-Kitchens-100.",
  "s2id": "54423c91d0db22193439f21d424302eab2d83b24",
  "citations": 0
}