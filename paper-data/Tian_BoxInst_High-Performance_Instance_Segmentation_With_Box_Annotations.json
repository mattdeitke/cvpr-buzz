{
  "arXiv": "http://arxiv.org/abs/2012.02310",
  "title": "BoxInst: High-Performance Instance Segmentation With Box Annotations",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Tian_BoxInst_High-Performance_Instance_Segmentation_With_Box_Annotations_CVPR_2021_paper.pdf",
  "authors": [
    "Zhi Tian",
    "Chunhua Shen",
    "Xinlong Wang",
    "Hao Chen"
  ],
  "abstract": "We present a high-performance method that can achieve mask-level instance segmentation with only bounding-box annotations for training. While this setting has been studied in the literature, here we show significantly stronger performance with a simple design (e.g., dramatically improving previous best reported mask AP of 21.1% to 31.6% on the COCO dataset). Our core idea is to redesign the loss of learning masks in instance segmentation, with no modification to the segmentation network itself. The new loss functions can supervise the mask training without relying on mask annotations. This is made possible with two loss terms, namely, 1) a surrogate term that minimizes the discrepancy between the projections of the ground-truth box and the predicted mask; 2) a pairwise loss that can exploit the prior that proximal pixels with similar colors are very likely to have the same category label. Experiments demonstrate that the redesigned mask loss can yield surprisingly high-quality instance masks with only box annotations. For example, without using any mask annotations, with a ResNet-101 backbone and 3x training schedule, we achieve 33.2% mask AP on COCO test-dev split (vs. 39.1% of the fully supervised counterpart). Our excellent experiment results on COCO and Pascal VOC indicate that our method dramatically narrows the performance gap between weakly and fully supervised instance segmentation. Code is available at https://git.io/AdelaiDet",
  "s2id": "543600e3c933c843704b551a6e0bcd28cfd50bed",
  "twitter": {
    "retweets": 24,
    "likes": 102,
    "replies": 0
  },
  "citations": 4,
  "posterSession": "Tuesday"
}