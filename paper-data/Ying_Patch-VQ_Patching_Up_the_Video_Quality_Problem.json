{
  "arXiv": null,
  "title": "Patch-VQ: 'Patching Up' the Video Quality Problem",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Ying_Patch-VQ_Patching_Up_the_Video_Quality_Problem_CVPR_2021_paper.pdf",
  "authors": [
    "Zhenqiang Ying",
    "Maniratnam Mandal",
    "Deepti Ghadiyaram",
    "Alan Bovik"
  ],
  "abstract": "No-reference (NR) perceptual video quality assessment (VQA) is a complex, unsolved, and important problem for social and streaming media applications. Efficient and accurate video quality predictors are needed to monitor and guide the processing of billions of shared, often imperfect, user-generated content (UGC). Unfortunately, current NR models are limited in their prediction capabilities on real-world, \"in-the-wild\" UGC video data. To advance progress on this problem, we created the largest (by far) subjective video quality dataset, containing 38,811 real-world distorted videos and 116,433 space-time localized video patches ('v-patches'), and 5.5M human perceptual quality annotations. Using this, we created two unique NR-VQA models: (a) a local-to-global region-based NR VQA architecture (called PVQ) that learns to predict global video quality and achieves state-of-the-art performance on 3 UGC datasets, and (b) a first-of-a-kind space-time video quality mapping engine (called PVQ Mapper) that helps localize and visualize perceptual distortions in space and time. The entire dataset and prediction models are freely available at https://live.ece.utexas.edu/research.php.",
  "s2id": "3488600d290397d8869634d893edd642bf8f6e15",
  "twitter": {
    "retweets": 0,
    "likes": 3,
    "replies": 0,
    "ids": [
      "1333517414802350083",
      "1333420864918990850",
      "1333239644998295552"
    ]
  },
  "citations": 2,
  "posterSession": "Thursday"
}