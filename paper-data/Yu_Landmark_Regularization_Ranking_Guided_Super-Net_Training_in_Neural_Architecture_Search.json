{
  "arXiv": "http://arxiv.org/abs/2104.05309",
  "title": "Landmark Regularization: Ranking Guided Super-Net Training in Neural Architecture Search",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Yu_Landmark_Regularization_Ranking_Guided_Super-Net_Training_in_Neural_Architecture_Search_CVPR_2021_paper.pdf",
  "authors": [
    "Kaicheng Yu",
    "Rene Ranftl",
    "Mathieu Salzmann"
  ],
  "abstract": "Weight sharing has become a de facto standard in neural architecture search because it enables the search to be done on commodity hardware. However, recent works have empirically shown a ranking disorder between the performance of stand-alone architectures and that of the corresponding shared-weight networks. This violates the main assumption of weight-sharing NAS algorithms, thus limiting their effectiveness. We tackle this issue by proposing a regularization term that aims to maximize the correlation between the performance rankings of the shared-weight network and that of the standalone architectures using a small set of landmark architectures. We incorporate our regularization term into three different NAS algorithms and show that it consistently improves performance across algorithms, search-spaces, and tasks.",
  "s2id": "b1d5e323d7303416dbb832ca70bc554d1a11f8c1",
  "twitter": {
    "retweets": 0,
    "likes": 0,
    "replies": 1
  },
  "citations": 0,
  "posterSession": "Thursday"
}