{
  "arXiv": "http://arxiv.org/abs/2104.00332",
  "title": "UC2: Universal Cross-Lingual Cross-Modal Vision-and-Language Pre-Training",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Zhou_UC2_Universal_Cross-Lingual_Cross-Modal_Vision-and-Language_Pre-Training_CVPR_2021_paper.pdf",
  "authors": [
    "Mingyang Zhou",
    "Luowei Zhou",
    "Shuohang Wang",
    "Yu Cheng",
    "Linjie Li",
    "Zhou Yu",
    "Jingjing Liu"
  ],
  "abstract": "Vision-and-language pre-training has achieved impressive success in learning multimodal representations between vision and language. To generalize this success to non-English languages, we introduce UC^2, the first machine translation-augmented framework for cross-lingual cross-modal representation learning. To tackle the scarcity problem of multilingual captions for image datasets, we first augment existing English-only datasets with other languages via machine translation (MT). Then we extend the standard Masked Language Modeling and Image-Text Matching training objectives to multilingual setting, where alignment between different languages is captured through shared visual context (eg. using image as pivot). To facilitate the learning of a joint embedding space of images and all languages of interest, we further propose two novel pre-training tasks, namely Maksed Region-to-Token Modeling (MRTM) and Visual Translation Language Modeling (VTLM), leveraging MT-enhanced translated data. Evaluation on multilingual image-text retrieval and multilingual visual question answering benchmarks demonstrates that our proposed framework achieves new state of the art on diverse non-English benchmarks while maintaining comparable performance to monolingual pre-trained models on English tasks.",
  "s2id": "70a79ded7818ba8ae807102b00643e331e344ee8",
  "twitter": {
    "retweets": 4,
    "likes": 40,
    "replies": 0
  },
  "citations": 0,
  "posterSession": "Tuesday"
}