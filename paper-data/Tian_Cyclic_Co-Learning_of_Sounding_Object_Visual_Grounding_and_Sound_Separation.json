{
  "arXiv": "http://arxiv.org/abs/2104.02026",
  "title": "Cyclic Co-Learning of Sounding Object Visual Grounding and Sound Separation",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Tian_Cyclic_Co-Learning_of_Sounding_Object_Visual_Grounding_and_Sound_Separation_CVPR_2021_paper.pdf",
  "authors": [
    "Yapeng Tian",
    "Di Hu",
    "Chenliang Xu"
  ],
  "abstract": "There are rich synchronized audio and visual events in our daily life. Inside the events, audio scenes are associated with the corresponding visual objects; meanwhile, sounding objects can indicate and help to separate their individual sounds in the audio track. Based on this observation, in this paper, we propose a cyclic co-learning (CCoL) paradigm that can jointly learn sounding object visual grounding and audio-visual sound separation in a unified framework. Concretely, we can leverage grounded object-sound relations to improve the results of sound separation. Meanwhile, benefiting from discriminative information from separated sounds, we improve training example sampling for sounding object grounding, which builds a co-learning cycle for the two tasks and makes them mutually beneficial. Extensive experiments show that the proposed framework outperforms the compared recent approaches on both tasks, and they can benefit from each other with our cyclic co-learning. The source code and pre-trained models are released in https://github.com/YapengTian/CCOL-CVPR21.",
  "s2id": "022b59b652b6f10f832144d622a2cc6beb6290af",
  "citations": 4,
  "posterSession": "Monday"
}