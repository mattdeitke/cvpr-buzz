{
  "arXiv": "http://arxiv.org/abs/2101.00529",
  "title": "VinVL: Revisiting Visual Representations in Vision-Language Models",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_VinVL_Revisiting_Visual_Representations_in_Vision-Language_Models_CVPR_2021_paper.pdf",
  "authors": [
    "Pengchuan Zhang",
    "Xiujun Li",
    "Xiaowei Hu",
    "Jianwei Yang",
    "Lei Zhang",
    "Lijuan Wang",
    "Yejin Choi",
    "Jianfeng Gao"
  ],
  "abstract": "This paper presents a detailed study of improving vision features and develops an improved object detection model for vision language (VL) tasks. Compared to the most widely used bottom-up and top-down model [2], the new model is bigger, pre-trained on much larger training corpora that combine multiple public annotated object detection datasets, and thus can generate representations of a richer collection of visual objects and concepts. While previous VL research focuses solely on improving the vision-language fusion model and leaves the object detection model improvement untouched, we present an empirical study to show that vision features matter significantly in VL models. In our experiments we feed the vision features generated by the new object detection model into a pre-trained transformer-based VL fusion model Oscar+, and fine-tune Oscar+ on a wide range of downstream VL tasks. Our results show that the new vision features significantly improve the performance across all VL tasks, creating new state-of-the-art results on seven public benchmarks. We will release the new object detection model to public.",
  "s2id": "",
  "twitter": {
    "retweets": 19,
    "likes": 95,
    "replies": 2
  },
  "posterSession": "Tuesday"
}