{
  "arXiv": "http://arxiv.org/abs/2012.15840",
  "title": "Rethinking Semantic Segmentation From a Sequence-to-Sequence Perspective With Transformers",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Zheng_Rethinking_Semantic_Segmentation_From_a_Sequence-to-Sequence_Perspective_With_Transformers_CVPR_2021_paper.pdf",
  "authors": [
    "Sixiao Zheng",
    "Jiachen Lu",
    "Hengshuang Zhao",
    "Xiatian Zhu",
    "Zekun Luo",
    "Yabiao Wang",
    "Yanwei Fu",
    "Jianfeng Feng",
    "Tao Xiang",
    "Philip H.S. Torr",
    "Li Zhang"
  ],
  "abstract": "Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoder-decoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts with larger receptive fields. Since context modeling is critical for segmentation, the latest efforts have been focused on increasing the receptive field, through either dilated/atrous convolutions or inserting attention modules. However, the encoder-decoder based FCN architecture remains unchanged. In this paper, we aim to provide an alternative perspective by treating semantic segmentation as a sequence-to-sequence prediction task. Specifically, we deploy a pure transformer (i.e., without convolution and resolution reduction) to encode an image as a sequence of patches. With the global context modeled in every layer of the transformer, this encoder can be combined with a simple decoder to provide a powerful segmentation model, termed SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state of the art on ADE20K (50.28% mIoU), Pascal Context (55.83% mIoU) and competitive results on Cityscapes. Particularly, we achieve the first position in the highly competitive ADE20K test server leaderboard on the day of submission.",
  "s2id": "8a3669af72b04e4bd462900c813b6fea11538eae",
  "twitter": {
    "retweets": 53,
    "likes": 201,
    "replies": 0
  },
  "citations": 52
}