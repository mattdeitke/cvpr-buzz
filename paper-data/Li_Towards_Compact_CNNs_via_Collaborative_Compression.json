{
  "arXiv": "http://arxiv.org/abs/2105.11228",
  "title": "Towards Compact CNNs via Collaborative Compression",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Towards_Compact_CNNs_via_Collaborative_Compression_CVPR_2021_paper.pdf",
  "authors": [
    "Yuchao Li",
    "Shaohui Lin",
    "Jianzhuang Liu",
    "Qixiang Ye",
    "Mengdi Wang",
    "Fei Chao",
    "Fan Yang",
    "Jincheng Ma",
    "Qi Tian",
    "Rongrong Ji"
  ],
  "abstract": "Channel pruning and tensor decomposition have received extensive attention in convolutional neural network compression. However, these two techniques are traditionally deployed in an isolated manner, leading to significant accuracy drop when pursuing high compression rates. In this paper, we propose a Collaborative Compression (CC) scheme, which joints channel pruning and tensor decomposition to compress CNN models by simultaneously learning the model sparsity and low-rankness. Specifically, we first investigate the compression sensitivity of each layer in the network, and then propose a Global Compression Rate Optimization that transforms the decision problem of compression rate into an optimization problem. After that, we propose multi-step heuristic compression to remove redundant compression units step-by-step, which fully considers the effect of the remaining compression space (i.e., unremoved compression units). Our method demonstrates superior performance gains over previous ones on various datasets and backbone architectures. For example, we achieve 52.9% FLOPs reduction by removing 48.4% parameters on ResNet-50 with only a Top-1 accuracy drop of 0.56% on ImageNet 2012.",
  "s2id": "9c58550759a7aa9924228273f41ba946eee55aca",
  "citations": 0
}