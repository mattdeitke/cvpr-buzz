{
  "arXiv": "http://arxiv.org/abs/2105.11541",
  "title": "Learning Better Visual Dialog Agents With Pretrained Visual-Linguistic Representation",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Tu_Learning_Better_Visual_Dialog_Agents_With_Pretrained_Visual-Linguistic_Representation_CVPR_2021_paper.pdf",
  "authors": [
    "Tao Tu",
    "Qing Ping",
    "Govindarajan Thattai",
    "Gokhan Tur",
    "Prem Natarajan"
  ],
  "abstract": "GuessWhat?! is a visual dialog guessing game which incorporates a Questioner agent that generates a sequence of questions, while an Oracle agent answers the respective questions about a target object in an image. Based on this dialog history between the Questioner and the Oracle, a Guesser agent makes a final guess of the target object. While previous work has focused on dialogue policy optimization and visual-linguistic information fusion, most work learns the vision-linguistic encoding for the three agents solely on the GuessWhat?! dataset without shared and prior knowledge of vision-linguistic representation. To bridge these gaps, this paper proposes new Oracle, Guesser and Questioner models that take advantage of a pretrained vision-linguistic model, VilBert. For Oracle model, we introduce a two-way background/target fusion mechanism to understand both intra and inter-object questions. For Guesser model, we introduce a state-estimator that best utilizes Vilbert's strength in single-turn referring expression comprehension. For the Questioner, we share the state-estimator from pretrained Guesser with Questioner to guide the question generator. Experimental results show that our proposed models outperform state-of-the-art models significantly by 7%, 10%, 12% for Oracle, Guesser and End-to-End Questioner respectively.",
  "s2id": "07afd0417be76aca92d406cff1346608ad5391ca",
  "twitter": {
    "retweets": 1,
    "likes": 6,
    "replies": 2
  },
  "citations": 0
}