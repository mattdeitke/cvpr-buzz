{
  "arXiv": "http://arxiv.org/abs/2012.03796",
  "title": "Pose-Guided Human Animation From a Single Image in the Wild",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Yoon_Pose-Guided_Human_Animation_From_a_Single_Image_in_the_Wild_CVPR_2021_paper.pdf",
  "authors": [
    "Jae Shin Yoon",
    "Lingjie Liu",
    "Vladislav Golyanik",
    "Kripasindhu Sarkar",
    "Hyun Soo Park",
    "Christian Theobalt"
  ],
  "abstract": "We present a new pose transfer method for synthesizing a human animation from a single image of a person controlled by a sequence of body poses. Existing pose transfer methods exhibit significant visual artifacts when applying to a novel scene, resulting in temporal inconsistency and failures in preserving the identity and textures of the person. To address these limitations, we design a compositional neural network that predicts the silhouette, garment labels, and textures. Each modular network is explicitly dedicated to a subtask that can be learned from the synthetic data. At the inference time, we utilize the trained network to produce a unified representation of appearance and its labels in UV coordinates, which remain constant across poses. The unified representation provides incomplete yet strong guidance to generating the appearance in response to the pose change. We use the trained network to complete the appearance and render it with the background. With these strategies, we are able to synthesize human animations that can preserve the identity and appearance of the person in a temporally coherent way without any fine-tuning of the network on the testing scene. Experiments show that our method outperforms the state-of-the-arts in terms of synthesis quality, temporal coherence, and generalization ability.",
  "s2id": "c89ae90f158a55c6a8faf363111d316f7ece99af",
  "twitter": {
    "retweets": 5,
    "likes": 20,
    "replies": 1
  },
  "citations": 2,
  "posterSession": "Thursday"
}