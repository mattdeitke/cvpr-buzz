{
  "arXiv": "http://arxiv.org/abs/2104.02439",
  "title": "Few-Shot Transformation of Common Actions Into Time and Space",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Few-Shot_Transformation_of_Common_Actions_Into_Time_and_Space_CVPR_2021_paper.pdf",
  "authors": [
    "Pengwan Yang",
    "Pascal Mettes",
    "Cees G. M. Snoek"
  ],
  "abstract": "This paper introduces the task of few-shot common action localization in time and space. Given a few trimmed support videos containing the same but unknown action, we strive for spatio-temporal localization of that action in a long untrimmed query video. We do not require any class labels, interval bounds, or bounding boxes. To address this challenging task, we introduce a novel few-shot transformer architecture with a dedicated encoder-decoder structure optimized for joint commonality learning and localization prediction, without the need for proposals. Experiments on our reorganizations of the AVA and UCF101-24 datasets show the effectiveness of our approach for few-shot common action localization, even when the support videos are noisy. Although we are not specifically designed for common localization in time only, we also compare favorably against the few-shot and one-shot state-of-the-art in this setting. Lastly, we demonstrate that the few-shot transformer is easily extended to common action localization per pixel.",
  "s2id": "f24cbc1f451a3de124d95bacd5985b1489f83ed8",
  "twitter": {
    "retweets": 23,
    "likes": 117,
    "replies": 1,
    "ids": [
      "1378369432528023552",
      "1379889024819621889",
      "1379597253346799616"
    ]
  },
  "citations": 0,
  "posterSession": "Friday"
}