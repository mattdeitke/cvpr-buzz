{
  "arXiv": null,
  "title": "Intelligent Carpet: Inferring 3D Human Pose From Tactile Signals",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Luo_Intelligent_Carpet_Inferring_3D_Human_Pose_From_Tactile_Signals_CVPR_2021_paper.pdf",
  "authors": [
    "Yiyue Luo",
    "Yunzhu Li",
    "Michael Foshey",
    "Wan Shou",
    "Pratyusha Sharma",
    "Tomas Palacios",
    "Antonio Torralba",
    "Wojciech Matusik"
  ],
  "abstract": "Daily human activities, e.g., locomotion, exercises, and resting, are heavily guided by the tactile interactions between the human and the ground. In this work, leveraging such tactile interactions, we propose a 3D human pose estimation approach using the pressure maps recorded by a tactile carpet as input. We build a low-cost, high-density, large-scale intelligent carpet, which enables the real-time recordings of human-floor tactile interactions in a seamless manner. We collect a synchronized tactile and visual dataset on various human activities. Employing a state-of-the-art camera-based pose estimation model as supervision, we design and implement a deep neural network model to infer 3D human poses using only the tactile information. Our pipeline can be further scaled up to multi-person pose estimation. We evaluate our system and demonstrate its potential applications in diverse fields.",
  "s2id": ""
}