{
  "arXiv": "http://arxiv.org/abs/2103.04337",
  "title": "Watching You: Global-Guided Reciprocal Learning for Video-Based Person Re-Identification",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Watching_You_Global-Guided_Reciprocal_Learning_for_Video-Based_Person_Re-Identification_CVPR_2021_paper.pdf",
  "authors": [
    "Xuehu Liu",
    "Pingping Zhang",
    "Chenyang Yu",
    "Huchuan Lu",
    "Xiaoyun Yang"
  ],
  "abstract": "Video-based person re-identification (Re-ID) aims to automatically retrieve video sequences of the same person under non-overlapping cameras. To achieve this goal, it is the key to fully utilize abundant spatial and temporal cues in videos. Existing methods usually focus on the most conspicuous image regions, thus they may easily miss out fine-grained clues due to the person varieties in image sequences. To address above issues, in this paper, we propose a novel Global-guided Reciprocal Learning (GRL) framework for video-based person Re-ID. Specifically, we first propose a Global-guided Correlation Estimation (GCE) to generate feature correlation maps of local features and global features, which help to localize the high- and low-correlation regions for identifying the same person. After that, the discriminative features are disentangled into high-correlation features and low-correlation features under the guidance of the global representations. Moreover, a novel Temporal Reciprocal Learning (TRL) mechanism is designed to sequentially enhance the high-correlation semantic information and accumulate the low-correlation sub-critical clues. Extensive experiments are conducted on three public benchmarks. The experimental results indicate that our approach can achieve better performance than other state-of-the-art approaches. The code is released at https://github.com/flysnowtiger/GRL.",
  "s2id": "2ff07e55f61a65bf423cc9b3bc1e7c6b2011b793",
  "citations": 2,
  "posterSession": "Thursday"
}