{
  "arXiv": "http://arxiv.org/abs/2103.00887",
  "title": "Counterfactual Zero-Shot and Open-Set Visual Recognition",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Yue_Counterfactual_Zero-Shot_and_Open-Set_Visual_Recognition_CVPR_2021_paper.pdf",
  "authors": [
    "Zhongqi Yue",
    "Tan Wang",
    "Qianru Sun",
    "Xian-Sheng Hua",
    "Hanwang Zhang"
  ],
  "abstract": "We present a novel counterfactual framework for both Zero-Shot Learning (ZSL) and Open-Set Recognition (OSR), whose common challenge is generalizing to the unseen-classes by only training on the seen-classes. Our idea stems from the observation that the generated samples for unseen-classes are often out of the true distribution, which causes severe recognition rate imbalance between the seen-class (high) and unseen-class (low). We show that the key reason is that the generation is not Counterfactual Faithful, and thus we propose a faithful one, whose generation is from the sample-specific counterfactual question: What would the sample look like, if we set its class attribute to a certain class, while keeping its sample attribute unchanged? Thanks to the faithfulness, we can apply the Consistency Rule to perform unseen/seen binary classification, by asking: Would its counterfactual still look like itself? If \"yes\", the sample is from a certain class, and \"no\" otherwise. Through extensive experiments on ZSL and OSR, we demonstrate that our framework effectively mitigates the seen/unseen imbalance and hence significantly improves the overall performance. Note that this framework is orthogonal to existing methods, thus, it can serve as a new baseline to evaluate how ZSL/OSR models generalize. Codes are available at https://github.com/yue-zhongqi/gcm-cf.",
  "s2id": "1d3258abdfdc9262a3c5736a4619869a39d17120",
  "twitter": {
    "retweets": 3,
    "likes": 16,
    "replies": 0
  },
  "citations": 3
}