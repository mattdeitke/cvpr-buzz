{
  "arXiv": "http://arxiv.org/abs/2102.13090",
  "title": "IBRNet: Learning Multi-View Image-Based Rendering",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_IBRNet_Learning_Multi-View_Image-Based_Rendering_CVPR_2021_paper.pdf",
  "authors": [
    "Qianqian Wang",
    "Zhicheng Wang",
    "Kyle Genova",
    "Pratul P. Srinivasan",
    "Howard Zhou",
    "Jonathan T. Barron",
    "Ricardo Martin-Brualla",
    "Noah Snavely",
    "Thomas Funkhouser"
  ],
  "abstract": "We present a method that synthesizes novel views of complex scenes by interpolating a sparse set of nearby views. The core of our method is a network architecture that includes a multilayer perceptron and a ray transformer that estimates radiance and volume density at continuous 5D locations (3D spatial locations and 2D viewing directions), drawing appearance information on the fly from multiple source views. By drawing on source views at render time, our method hearkens back to classic work on image-based rendering (IBR), and allows us to render high-resolution imagery. Unlike neural scene representation work that optimizes per-scene functions for rendering, we learn a generic view interpolation function that generalizes to novel scenes. We render images using classic volume rendering, which is fully differentiable and allows us to train using only multi-view posed images as supervision. Experiments show that our method outperforms recent novel view synthesis methods that also seek to generalize to novel scenes. Further, if fine-tuned on each scene, our method is competitive with state-of-the-art single-scene neural rendering methods.",
  "s2id": "7cbc3dd0280b8c4551ac934af42dc227d43754f7",
  "twitter": {
    "retweets": 61,
    "likes": 343,
    "replies": 3
  },
  "citations": 11,
  "posterSession": "Tuesday"
}