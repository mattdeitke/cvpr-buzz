{
  "arXiv": "http://arxiv.org/abs/2103.11624",
  "title": "Multimodal Motion Prediction With Stacked Transformers",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Multimodal_Motion_Prediction_With_Stacked_Transformers_CVPR_2021_paper.pdf",
  "authors": [
    "Yicheng Liu",
    "Jinghuai Zhang",
    "Liangji Fang",
    "Qinhong Jiang",
    "Bolei Zhou"
  ],
  "abstract": "Predicting multiple plausible future trajectories of the nearby vehicles is crucial for the safety of autonomous driving. Recent motion prediction approaches attempt to achieve such multimodal motion prediction by implicitly regularizing the feature or explicitly generating multiple candidate proposals. However, it remains challenging since the latent features may concentrate on the most frequent mode of the data while the proposal-based methods depend largely on the prior knowledge to generate and select the proposals. In this work, we propose a novel transformer framework for multimodal motion prediction, termed as mmTransformer. A novel network architecture based on stacked transformers is designed to model the multimodality at feature level with a set of fixed independent proposals. A region-based training strategy is then developed to induce the multimodality of the generated proposals. Experiments on Argoverse dataset show that the proposed model achieves the state-of-the-art performance on motion prediction, substantially improving the diversity and the accuracy of the predicted trajectories. Demo video and code are available at https://decisionforce. github.io/mmTransformer.",
  "s2id": "22ad3545d78f2acfe7de1b2c38ec72efc9faa0d6",
  "twitter": {
    "retweets": 35,
    "likes": 245,
    "replies": 2
  },
  "citations": 2
}