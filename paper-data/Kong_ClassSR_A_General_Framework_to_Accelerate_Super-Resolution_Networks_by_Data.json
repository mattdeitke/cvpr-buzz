{
  "arXiv": "http://arxiv.org/abs/2103.04039",
  "title": "ClassSR: A General Framework to Accelerate Super-Resolution Networks by Data Characteristic",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Kong_ClassSR_A_General_Framework_to_Accelerate_Super-Resolution_Networks_by_Data_CVPR_2021_paper.pdf",
  "authors": [
    "Xiangtao Kong",
    "Hengyuan Zhao",
    "Yu Qiao",
    "Chao Dong"
  ],
  "abstract": "We aim at accelerating super-resolution (SR) networks on large images (2K-8K). The large images are usually decomposed into small sub-images in practical usages. Based on this processing, we found that different image regions have different restoration difficulties and can be processed by networks with different capacities. Intuitively, smooth areas are easier to super-solve than complex textures. To utilize this property, we can adopt appropriate SR networks to process different sub-images after the decomposition. On this basis, we propose a new solution pipeline -- ClassSR that combines classification and SR in a unified framework. In particular, it first uses a Class-Module to classify the sub-images into different classes according to restoration difficulties, then applies an SR-Module to perform SR for different classes. The Class-Module is a conventional classification network, while the SR-Module is a network container that consists of the to-be-accelerated SR network and its simplified versions. We further introduce a new classification method with two losses -- Class-Loss and Average-Loss to produce the classification results. After joint training, a majority of sub-images will pass through smaller networks, thus the computational cost can be significantly reduced. Experiments show that our ClassSR can help most existing methods (e.g., FSRCNN, CARN, SRResNet, RCAN) save up to 50% FLOPs on DIV8K datasets. This general framework can also be applied in other low-level vision tasks.",
  "s2id": "1bc2232a8d6e210a3ccefc473a414d6d9c08d003",
  "twitter": {
    "retweets": 9,
    "likes": 21,
    "replies": 2
  },
  "citations": 0
}