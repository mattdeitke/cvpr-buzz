{
  "arXiv": "http://arxiv.org/abs/2011.11814",
  "title": "MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments From a Single Moving Camera",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Wimbauer_MonoRec_Semi-Supervised_Dense_Reconstruction_in_Dynamic_Environments_From_a_Single_CVPR_2021_paper.pdf",
  "authors": [
    "Felix Wimbauer",
    "Nan Yang",
    "Lukas von Stumberg",
    "Niclas Zeller",
    "Daniel Cremers"
  ],
  "abstract": "In this paper, we propose MonoRec, a semi-supervised monocular dense reconstruction architecture that predicts depth maps from a single moving camera in dynamic environments. MonoRec is based on a multi-view stereo setting which encodes the information of multiple consecutive images in a cost volume. To deal with dynamic objects in the scene, we introduce a MaskModule that predicts moving object masks by leveraging the photometric inconsistencies encoded in the cost volumes. Unlike other multi-view stereo methods, MonoRec is able to reconstruct both static and moving objects by leveraging the predicted masks. Furthermore, we present a novel multi-stage training scheme with a semi-supervised loss formulation that does not require LiDAR depth values. We carefully evaluate MonoRec on the KITTI dataset and show that it achieves state-of-the-art performance compared to both multi-view and single-view methods. With the model trained on KITTI, we further demonstrate that MonoRec is able to generalize well to both the Oxford RobotCar dataset and the more challenging TUM-Mono dataset recorded by a handheld camera. Code and related materials are available at https://vision.in.tum.de/research/monorec.",
  "s2id": "44c1149ba836a479b389fba4412a29340208c7d0",
  "twitter": {
    "retweets": 59,
    "likes": 399,
    "replies": 0
  },
  "citations": 1
}