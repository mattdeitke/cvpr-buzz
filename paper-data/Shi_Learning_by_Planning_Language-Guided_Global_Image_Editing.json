{
  "arXiv": null,
  "title": "Learning by Planning: Language-Guided Global Image Editing",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Shi_Learning_by_Planning_Language-Guided_Global_Image_Editing_CVPR_2021_paper.pdf",
  "authors": [
    "Jing Shi",
    "Ning Xu",
    "Yihang Xu",
    "Trung Bui",
    "Franck Dernoncourt",
    "Chenliang Xu"
  ],
  "abstract": "Recently, language-guided global image editing draws increasing attention with growing application potentials. However, previous GAN-based methods are not only confined to domain-specific, low-resolution data but also lacking in interpretability. To overcome the collective difficulties, we develop a text-to-operation model to map the vague editing language request into a series of editing operations, e.g., change contrast, brightness, and saturation. Each operation is interpretable and differentiable. Furthermore, the only supervision in the task is the target image, which is insufficient for a stable training of sequential decisions. Hence, we propose a novel operation planning algorithm to generate possible editing sequences from the target image as pseudo ground truth. Comparison experiments on the newly collected MA5k-Req dataset and GIER dataset show the advantages of our methods. Code is available at https://github.com/jshi31/T2ONet.",
  "s2id": "aaeeeecc0926a9d3f6f4784c7ad071300b90dec7",
  "citations": 0,
  "posterSession": "Thursday"
}