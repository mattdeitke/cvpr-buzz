{
  "arXiv": "http://arxiv.org/abs/2103.07156",
  "title": "Learnable Companding Quantization for Accurate Low-Bit Neural Networks",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Yamamoto_Learnable_Companding_Quantization_for_Accurate_Low-Bit_Neural_Networks_CVPR_2021_paper.pdf",
  "authors": [
    "Kohei Yamamoto"
  ],
  "abstract": "Quantizing deep neural networks is an effective method for reducing memory consumption and improving inference speed, and is thus useful for implementation in resource-constrained devices. However, it is still hard for extremely low-bit models to achieve accuracy comparable with that of full-precision models. To address this issue, we propose learnable companding quantization (LCQ) as a novel non-uniform quantization method for 2-, 3-, and 4-bit models. LCQ jointly optimizes model weights and learnable companding functions that can flexibly and non-uniformly control the quantization levels of weights and activations. We also present a new weight normalization technique that allows more stable training for quantization. Experimental results show that LCQ outperforms conventional state-of-the-art methods and narrows the gap between quantized and full-precision models for image classification and object detection tasks. Notably, the 2-bit ResNet-50 model on ImageNet achieves top-1 accuracy of 75.1% and reduces the gap to 1.7%, allowing LCQ to further exploit the potential of non-uniform quantization.",
  "s2id": "62d1ad6f8cdaa80e995f24a8871397118f050b5a",
  "twitter": {
    "retweets": 2,
    "likes": 3,
    "replies": 1,
    "ids": [
      "1371433024005365768",
      "1371275077753405441",
      "1373756058158645251",
      "1371456368192991239",
      "1371356192723726336"
    ]
  },
  "citations": 0,
  "posterSession": "Tuesday"
}