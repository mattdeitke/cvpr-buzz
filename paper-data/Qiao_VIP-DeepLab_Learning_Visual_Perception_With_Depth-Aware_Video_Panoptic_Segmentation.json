{
  "arXiv": null,
  "title": "VIP-DeepLab: Learning Visual Perception With Depth-Aware Video Panoptic Segmentation",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Qiao_VIP-DeepLab_Learning_Visual_Perception_With_Depth-Aware_Video_Panoptic_Segmentation_CVPR_2021_paper.pdf",
  "authors": [
    "Siyuan Qiao",
    "Yukun Zhu",
    "Hartwig Adam",
    "Alan Yuille",
    "Liang-Chieh Chen"
  ],
  "abstract": "In this paper, we present ViP-DeepLab, a unified model attempting to tackle the long-standing and challenging inverse projection problem in vision, which we model as restoring the point clouds from perspective image sequences while providing each point with instance-level semantic interpretations. Solving this problem requires the vision models to predict the spatial location, semantic class, and temporally consistent instance label for each 3D point. ViP-DeepLab approaches it by jointly performing monocular depth estimation and video panoptic segmentation. We name this joint task as Depth-aware Video Panoptic Segmentation, and propose a new evaluation metric along with two derived datasets for it, which will be made available to the public. On the individual sub-tasks, ViP-DeepLab also achieves state-of-the-art results, outperforming previous methods by 5.1% VPQ on Cityscapes-VPS, ranking 1st on the KITTI monocular depth estimation benchmark, and 1st on KITTI MOTS pedestrian. The datasets and the evaluation codes are made publicly available.",
  "s2id": "86f88bc71034122eb9d4f8ea16371ebd3efd42cc",
  "twitter": {
    "retweets": 8,
    "likes": 25,
    "replies": 1,
    "ids": [
      "1337212617186799618",
      "1337746843841662981",
      "1337271664971681795",
      "1387171788979802114",
      "1389595899664404490",
      "1389595822480891910",
      "1387181593324752896",
      "1341146669358977024"
    ]
  },
  "citations": 3,
  "posterSession": "Tuesday"
}