{
  "arXiv": null,
  "title": "Jo-SRC: A Contrastive Approach for Combating Noisy Labels",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Yao_Jo-SRC_A_Contrastive_Approach_for_Combating_Noisy_Labels_CVPR_2021_paper.pdf",
  "authors": [
    "Yazhou Yao",
    "Zeren Sun",
    "Chuanyi Zhang",
    "Fumin Shen",
    "Qi Wu",
    "Jian Zhang",
    "Zhenmin Tang"
  ],
  "abstract": "Due to the memorization effect in Deep Neural Networks (DNNs), training with noisy labels usually results in inferior model performance. Existing state-of-the-art methods primarily adopt a sample selection strategy, which selects small-loss samples for subsequent training. However, prior literature tends to perform sample selection within each mini-batch, neglecting the imbalance of noise ratios in different mini-batches. Moreover, valuable knowledge within high-loss samples is wasted. To this end, we propose a noise-robust approach named Jo-SRC (Joint Sample Selection and Model Regularization based on Consistency). Specifically, we train the network in a contrastive learning manner. Predictions from two different views of each sample are used to estimate its \"likelihood\" of being clean or out-of-distribution. Furthermore, we propose a joint loss to advance the model generalization performance by introducing consistency regularization. Extensive experiments and ablation studies have validated the superiority of our approach over existing state-of-the-art methods.",
  "s2id": "d128dde66b604534465c13e603b26762797c0138",
  "citations": 0
}