{
  "arXiv": "http://arxiv.org/abs/2102.08981",
  "title": "Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts",
  "pdf": "https://openaccess.thecvf.com/content/CVPR2021/papers/Changpinyo_Conceptual_12M_Pushing_Web-Scale_Image-Text_Pre-Training_To_Recognize_Long-Tail_Visual_CVPR_2021_paper.pdf",
  "authors": [
    "Soravit Changpinyo",
    "Piyush Sharma",
    "Nan Ding",
    "Radu Soricut"
  ],
  "abstract": "The availability of large-scale image captioning and visual question answering datasets has contributed significantly to recent successes in vision-and-language pre-training. However, these datasets are often collected with overrestrictive requirements inherited from their original target tasks (e.g., image caption generation), which limit the resulting dataset scale and diversity. We take a step further in pushing the limits of vision-and-language pre-training data by relaxing the data collection pipeline used in Conceptual Captions 3M (CC3M) [Sharma et al. 2018] and introduce the Conceptual 12M (CC12M), a dataset with 12 million image-text pairs specifically meant to be used for vision-and-language pre-training. We perform an analysis of this dataset and benchmark its effectiveness against CC3M on multiple downstream tasks with an emphasis on long-tail visual recognition. Our results clearly illustrate the benefit of scaling up pre-training data for vision-and-language tasks, as indicated by the new state-of-the-art results on both the nocaps and Conceptual Captions benchmarks.",
  "s2id": "394be105b87e9bfe72c20efe6338de10604e1a11",
  "twitter": {
    "retweets": 12,
    "likes": 48,
    "replies": 1,
    "ids": [
      "1362596321400672259",
      "1363529218861256711",
      "1377345441495359488",
      "1377254912812331012",
      "1377104037183594501",
      "1377073611442434050",
      "1363378074230943749",
      "1363348110378868740",
      "1363222877219209223",
      "1363181943706312704",
      "1362985614367481856",
      "1362789366217793545",
      "1362593008345108480"
    ]
  },
  "citations": 2,
  "posterSession": "Tuesday"
}